{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995fdd42",
   "metadata": {},
   "source": [
    "# AI Model Development for Football Analytics\n",
    "\n",
    "This notebook implements various machine learning models for football player performance prediction.\n",
    "\n",
    "## Contents\n",
    "1. Data Preparation and Splitting\n",
    "2. Baseline Models (Linear Regression, Random Forest)\n",
    "3. Advanced Models (XGBoost, Neural Networks)\n",
    "4. Time Series Models (Prophet, LSTM)\n",
    "5. Ensemble Methods\n",
    "6. Model Evaluation and Comparison\n",
    "7. Model Interpretation and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31dd79",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd12fef",
   "metadata": {},
   "source": [
    "## Data Processing & Feature Engineering\n",
    "\n",
    "This section implements comprehensive feature engineering for football analytics, leveraging the 2025/26 season data (4 rounds completed as of September 2025).\n",
    "\n",
    "### Feature Categories Overview:\n",
    "1. **Position-Specific Performance Metrics**\n",
    "2. **Form & Momentum Indicators** \n",
    "3. **Fixture Difficulty & Context**\n",
    "4. **Value & Ownership Dynamics**\n",
    "5. **Team Performance Integration**\n",
    "6. **Advanced Statistical Features**\n",
    "7. **Rolling Window Aggregations**\n",
    "8. **Seasonal Progression Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e164dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering for football analytics\n",
    "    Designed for 2025/26 season data with 4+ rounds completed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.position_mappings = {\n",
    "            1: 'GKP',  # Goalkeeper\n",
    "            2: 'DEF',  # Defender  \n",
    "            3: 'MID',  # Midfielder\n",
    "            4: 'FWD'   # Forward/Attacker\n",
    "        }\n",
    "        \n",
    "        self.difficulty_weights = {\n",
    "            1: 0.2, 2: 0.4, 3: 0.6, 4: 0.8, 5: 1.0  # FDR scale\n",
    "        }\n",
    "    \n",
    "    def load_data_from_db(self, db_path='data/fpl_data.db'):\n",
    "        \"\"\"Load data from SQLite database\"\"\"\n",
    "        import sqlite3\n",
    "        \n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Load players data\n",
    "        players_df = pd.read_sql_query(\"\"\"\n",
    "            SELECT * FROM players \n",
    "            ORDER BY gameweek, player_id\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        # Load teams data  \n",
    "        teams_df = pd.read_sql_query(\"\"\"\n",
    "            SELECT * FROM teams\n",
    "            ORDER BY gameweek, team_id\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        # Load matches data\n",
    "        matches_df = pd.read_sql_query(\"\"\"\n",
    "            SELECT * FROM matches\n",
    "            ORDER BY gameweek, match_id\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return players_df, teams_df, matches_df\n",
    "    \n",
    "    def create_position_specific_features(self, df):\n",
    "        \"\"\"\n",
    "        Enhanced position-specific metrics based on your requirements\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Map position codes to names\n",
    "        df['position_name'] = df['element_type'].map(self.position_mappings)\n",
    "        \n",
    "        # GOALKEEPERS - Enhanced metrics\n",
    "        gkp_features = []\n",
    "        if 'GKP' in df['position_name'].values:\n",
    "            # Core GKP metrics (your original plan)\n",
    "            df['save_percentage'] = np.where(\n",
    "                df['saves'] + df['goals_conceded'] > 0,\n",
    "                df['saves'] / (df['saves'] + df['goals_conceded']),\n",
    "                0\n",
    "            )\n",
    "            df['clean_sheet_rate'] = df['clean_sheets'] / df['minutes'].clip(lower=1) * 90\n",
    "            \n",
    "            # Enhanced GKP features\n",
    "            df['saves_per_90'] = df['saves'] / df['minutes'].clip(lower=1) * 90\n",
    "            df['penalties_saved_rate'] = df['penalties_saved'] / df['penalties_faced'].clip(lower=1)\n",
    "            df['goals_conceded_per_90'] = df['goals_conceded'] / df['minutes'].clip(lower=1) * 90\n",
    "            df['distribution_accuracy'] = df['passes_completed'] / df['passes_attempted'].clip(lower=1)\n",
    "            \n",
    "            # Advanced GKP metrics\n",
    "            df['gkp_value_rating'] = (\n",
    "                df['save_percentage'] * 0.3 +\n",
    "                df['clean_sheet_rate'] * 0.4 +\n",
    "                df['distribution_accuracy'] * 0.2 +\n",
    "                (1 - df['goals_conceded_per_90'] / 3) * 0.1  # Normalize goals conceded\n",
    "            )\n",
    "            \n",
    "            gkp_features = ['save_percentage', 'clean_sheet_rate', 'saves_per_90', \n",
    "                           'penalties_saved_rate', 'goals_conceded_per_90', \n",
    "                           'distribution_accuracy', 'gkp_value_rating']\n",
    "        \n",
    "        # DEFENDERS - Enhanced metrics\n",
    "        def_features = []\n",
    "        if 'DEF' in df['position_name'].values:\n",
    "            # Core DEF metrics (your original plan)\n",
    "            df['tackles_won_rate'] = df['tackles'] / (df['tackles'] + 1)  # Avoid div by zero\n",
    "            df['interceptions_per_90'] = df['interceptions'] / df['minutes'].clip(lower=1) * 90\n",
    "            df['aerial_duels_won_rate'] = df['aerial_duels_won'] / df['aerial_duels_attempted'].clip(lower=1)\n",
    "            df['pass_accuracy'] = df['passes_completed'] / df['passes_attempted'].clip(lower=1)\n",
    "            \n",
    "            # Enhanced DEF features\n",
    "            df['defensive_actions_per_90'] = (\n",
    "                df['tackles'] + df['interceptions'] + df['clearances']\n",
    "            ) / df['minutes'].clip(lower=1) * 90\n",
    "            \n",
    "            df['attacking_threat_def'] = (\n",
    "                df['goals_scored'] * 6 + \n",
    "                df['assists'] * 3 + \n",
    "                df['key_passes'] + \n",
    "                df['shots']\n",
    "            )\n",
    "            \n",
    "            df['defensive_reliability'] = (\n",
    "                df['clean_sheets'] * 0.4 +\n",
    "                (df['tackles'] + df['interceptions']) * 0.3 +\n",
    "                df['aerial_duels_won'] * 0.2 +\n",
    "                df['pass_accuracy'] * 0.1\n",
    "            )\n",
    "            \n",
    "            # Bonus point potential\n",
    "            df['def_bonus_potential'] = (\n",
    "                df['goals_scored'] * 2 +\n",
    "                df['assists'] +\n",
    "                df['clean_sheets'] +\n",
    "                df['defensive_actions_per_90'] / 10\n",
    "            )\n",
    "            \n",
    "            def_features = ['tackles_won_rate', 'interceptions_per_90', 'aerial_duels_won_rate',\n",
    "                           'pass_accuracy', 'defensive_actions_per_90', 'attacking_threat_def',\n",
    "                           'defensive_reliability', 'def_bonus_potential']\n",
    "        \n",
    "        # MIDFIELDERS - Enhanced metrics\n",
    "        mid_features = []\n",
    "        if 'MID' in df['position_name'].values:\n",
    "            # Core MID metrics (your original plan)\n",
    "            df['key_passes_per_90'] = df['key_passes'] / df['minutes'].clip(lower=1) * 90\n",
    "            df['dribbles_success_rate'] = df['dribbles_completed'] / df['dribbles_attempted'].clip(lower=1)\n",
    "            df['possession_impact'] = df['passes_attempted'] / df['minutes'].clip(lower=1) * 90\n",
    "            \n",
    "            # Enhanced MID features\n",
    "            df['creativity_index'] = (\n",
    "                df['key_passes'] * 2 +\n",
    "                df['assists'] * 3 +\n",
    "                df['dribbles_completed'] +\n",
    "                df['shots'] * 0.5\n",
    "            )\n",
    "            \n",
    "            df['work_rate_index'] = (\n",
    "                df['tackles'] +\n",
    "                df['interceptions'] +\n",
    "                df['passes_attempted'] / 20  # Scale down passes\n",
    "            )\n",
    "            \n",
    "            df['attacking_mid_potential'] = (\n",
    "                df['goals_scored'] * 5 +\n",
    "                df['assists'] * 3 +\n",
    "                df['key_passes'] * 1.5 +\n",
    "                df['shots'] * 0.8\n",
    "            )\n",
    "            \n",
    "            df['defensive_mid_value'] = (\n",
    "                df['tackles'] * 1.5 +\n",
    "                df['interceptions'] * 1.2 +\n",
    "                df['pass_accuracy'] * 10 +\n",
    "                df['possession_impact'] * 0.1\n",
    "            )\n",
    "            \n",
    "            # Box-to-box rating\n",
    "            df['box_to_box_rating'] = (\n",
    "                df['attacking_mid_potential'] * 0.4 +\n",
    "                df['defensive_mid_value'] * 0.4 +\n",
    "                df['creativity_index'] * 0.2\n",
    "            )\n",
    "            \n",
    "            mid_features = ['key_passes_per_90', 'dribbles_success_rate', 'possession_impact',\n",
    "                           'creativity_index', 'work_rate_index', 'attacking_mid_potential',\n",
    "                           'defensive_mid_value', 'box_to_box_rating']\n",
    "        \n",
    "        # FORWARDS/ATTACKERS - Enhanced metrics  \n",
    "        fwd_features = []\n",
    "        if 'FWD' in df['position_name'].values:\n",
    "            # Core FWD metrics (your original plan)\n",
    "            df['goals_per_90'] = df['goals_scored'] / df['minutes'].clip(lower=1) * 90\n",
    "            df['shot_conversion_rate'] = df['goals_scored'] / df['shots'].clip(lower=1)\n",
    "            \n",
    "            # Expected Goals calculation (simplified)\n",
    "            df['xG_per_shot'] = np.where(\n",
    "                df['shots'] > 0,\n",
    "                (df['shots_on_target'] * 0.3 + df['shots_off_target'] * 0.1) / df['shots'],\n",
    "                0\n",
    "            )\n",
    "            df['expected_goals'] = df['shots'] * df['xG_per_shot']\n",
    "            \n",
    "            # Enhanced FWD features\n",
    "            df['attacking_threat'] = (\n",
    "                df['shots'] * 1.5 +\n",
    "                df['shots_on_target'] * 2 +\n",
    "                df['key_passes'] +\n",
    "                df['assists'] * 3\n",
    "            )\n",
    "            \n",
    "            df['clinical_finishing'] = np.where(\n",
    "                df['expected_goals'] > 0,\n",
    "                df['goals_scored'] / df['expected_goals'],\n",
    "                df['shot_conversion_rate']\n",
    "            )\n",
    "            \n",
    "            df['penalty_specialist'] = (\n",
    "                df['penalties_scored'] / df['penalties_attempted'].clip(lower=1)\n",
    "            )\n",
    "            \n",
    "            df['big_chance_rating'] = (\n",
    "                df['goals_scored'] * 0.4 +\n",
    "                df['assists'] * 0.3 +\n",
    "                df['shot_conversion_rate'] * 10 * 0.2 +\n",
    "                df['attacking_threat'] * 0.1\n",
    "            )\n",
    "            \n",
    "            # Differential pick potential\n",
    "            df['fwd_differential_score'] = (\n",
    "                df['goals_per_90'] * df['minutes'] / 90 * \n",
    "                (1 - df['selected_by_percent'] / 100)  # Reward low ownership\n",
    "            )\n",
    "            \n",
    "            fwd_features = ['goals_per_90', 'shot_conversion_rate', 'xG_per_shot', 'expected_goals',\n",
    "                           'attacking_threat', 'clinical_finishing', 'penalty_specialist',\n",
    "                           'big_chance_rating', 'fwd_differential_score']\n",
    "        \n",
    "        # Store feature lists for later use\n",
    "        self.position_features = {\n",
    "            'GKP': gkp_features,\n",
    "            'DEF': def_features, \n",
    "            'MID': mid_features,\n",
    "            'FWD': fwd_features\n",
    "        }\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_form_momentum_features(self, df):\n",
    "        \"\"\"\n",
    "        Form and momentum indicators - crucial for FPL success\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(['player_id', 'gameweek'])\n",
    "        \n",
    "        # Rolling form windows (last 3, 5, 10 games)\n",
    "        windows = [3, 5, 10]\n",
    "        \n",
    "        for window in windows:\n",
    "            # Points form\n",
    "            df[f'points_form_{window}'] = (\n",
    "                df.groupby('player_id')['total_points']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Goals form\n",
    "            df[f'goals_form_{window}'] = (\n",
    "                df.groupby('player_id')['goals_scored']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .sum()\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Minutes consistency\n",
    "            df[f'minutes_consistency_{window}'] = (\n",
    "                df.groupby('player_id')['minutes']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .std()\n",
    "                .fillna(0)\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "        \n",
    "        # Momentum indicators\n",
    "        df['points_trend'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .diff()\n",
    "            .fillna(0)\n",
    "        )\n",
    "        \n",
    "        df['improving_form'] = (\n",
    "            df['points_form_3'] > df['points_form_5']\n",
    "        ).astype(int)\n",
    "        \n",
    "        df['declining_form'] = (\n",
    "            df['points_form_3'] < df['points_form_5']\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Hot streak detection\n",
    "        df['consecutive_returns'] = (\n",
    "            df.groupby('player_id')\n",
    "            .apply(lambda x: x['total_points'].gt(0).groupby((x['total_points'] <= 0).cumsum()).cumsum())\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Blank streak (important for transfers out)\n",
    "        df['consecutive_blanks'] = (\n",
    "            df.groupby('player_id')\n",
    "            .apply(lambda x: x['total_points'].eq(0).groupby((x['total_points'] > 0).cumsum()).cumsum())\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_fixture_context_features(self, df, matches_df):\n",
    "        \"\"\"\n",
    "        Fixture difficulty and context - essential for captain picks\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Merge with fixture data\n",
    "        df = df.merge(\n",
    "            matches_df[['gameweek', 'team_id', 'difficulty', 'home_away', 'opponent']],\n",
    "            on=['gameweek', 'team_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Home/away form splits\n",
    "        for venue in ['home', 'away']:\n",
    "            venue_mask = df['home_away'] == venue\n",
    "            \n",
    "            df[f'points_form_3_{venue}'] = (\n",
    "                df[venue_mask].groupby('player_id')['total_points']\n",
    "                .rolling(window=3, min_periods=1)\n",
    "                .mean()\n",
    "                .reindex(df.index)\n",
    "                .fillna(0)\n",
    "            )\n",
    "        \n",
    "        # Difficulty-adjusted metrics\n",
    "        df['difficulty_weight'] = df['difficulty'].map(self.difficulty_weights).fillna(0.6)\n",
    "        \n",
    "        df['difficulty_adjusted_points'] = (\n",
    "            df['total_points'] / df['difficulty_weight']\n",
    "        )\n",
    "        \n",
    "        # Fixture swing analysis\n",
    "        df['next_fixture_difficulty'] = (\n",
    "            df.groupby('player_id')['difficulty']\n",
    "            .shift(-1)\n",
    "            .fillna(3)  # Average difficulty\n",
    "        )\n",
    "        \n",
    "        df['fixture_swing'] = (\n",
    "            df['next_fixture_difficulty'] - df['difficulty']\n",
    "        )\n",
    "        \n",
    "        # Double gameweek potential (placeholder for future)\n",
    "        df['double_gameweek_potential'] = 0  # Will be updated when DGWs announced\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_value_ownership_features(self, df):\n",
    "        \"\"\"\n",
    "        Value and ownership dynamics - crucial for template avoidance\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Price change tracking\n",
    "        df['price_change'] = (\n",
    "            df.groupby('player_id')['now_cost']\n",
    "            .diff()\n",
    "            .fillna(0)\n",
    "        )\n",
    "        \n",
    "        df['price_change_total'] = (\n",
    "            df.groupby('player_id')['price_change']\n",
    "            .cumsum()\n",
    "        )\n",
    "        \n",
    "        # Value metrics\n",
    "        df['points_per_million'] = (\n",
    "            df['total_points'] / (df['now_cost'] / 10)\n",
    "        )\n",
    "        \n",
    "        df['value_form_3'] = (\n",
    "            df['points_form_3'] / (df['now_cost'] / 10)\n",
    "        )\n",
    "        \n",
    "        # Ownership dynamics\n",
    "        df['ownership_change'] = (\n",
    "            df.groupby('player_id')['selected_by_percent']\n",
    "            .diff()\n",
    "            .fillna(0)\n",
    "        )\n",
    "        \n",
    "        df['template_player'] = (\n",
    "            df['selected_by_percent'] > 50\n",
    "        ).astype(int)\n",
    "        \n",
    "        df['differential_player'] = (\n",
    "            df['selected_by_percent'] < 10\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Transfer activity\n",
    "        df['transfer_momentum'] = (\n",
    "            df['transfers_in'] - df['transfers_out']\n",
    "        )\n",
    "        \n",
    "        df['transfer_momentum_pct'] = (\n",
    "            df['transfer_momentum'] / df['selected_by_percent'].clip(lower=1) * 100\n",
    "        )\n",
    "        \n",
    "        # Value opportunity score\n",
    "        df['value_opportunity'] = (\n",
    "            df['points_per_million'] * \n",
    "            (1 - df['selected_by_percent'] / 100) *  # Lower ownership bonus\n",
    "            np.exp(-df['price_change_total'] / 5)    # Recent price rise penalty\n",
    "        )\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_team_integration_features(self, df, teams_df):\n",
    "        \"\"\"\n",
    "        Team performance integration - individual performance in team context\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Merge team performance\n",
    "        df = df.merge(\n",
    "            teams_df[['gameweek', 'team_id', 'strength_overall_home', 'strength_overall_away',\n",
    "                     'strength_attack_home', 'strength_attack_away', \n",
    "                     'strength_defence_home', 'strength_defence_away']],\n",
    "            on=['gameweek', 'team_id'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Team strength context\n",
    "        df['team_attack_strength'] = np.where(\n",
    "            df['home_away'] == 'home',\n",
    "            df['strength_attack_home'],\n",
    "            df['strength_attack_away']\n",
    "        )\n",
    "        \n",
    "        df['team_defence_strength'] = np.where(\n",
    "            df['home_away'] == 'home', \n",
    "            df['strength_defence_home'],\n",
    "            df['strength_defence_away']\n",
    "        )\n",
    "        \n",
    "        # Player contribution to team\n",
    "        team_totals = df.groupby(['gameweek', 'team_id']).agg({\n",
    "            'goals_scored': 'sum',\n",
    "            'assists': 'sum', \n",
    "            'total_points': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        team_totals.columns = ['gameweek', 'team_id', 'team_goals', 'team_assists', 'team_points']\n",
    "        \n",
    "        df = df.merge(team_totals, on=['gameweek', 'team_id'], how='left')\n",
    "        \n",
    "        # Contribution percentages\n",
    "        df['goal_contribution_pct'] = (\n",
    "            (df['goals_scored'] + df['assists']) / df['team_goals'].clip(lower=1) * 100\n",
    "        )\n",
    "        \n",
    "        df['points_contribution_pct'] = (\n",
    "            df['total_points'] / df['team_points'].clip(lower=1) * 100\n",
    "        )\n",
    "        \n",
    "        # Team performance indicators\n",
    "        df['team_over_performing'] = (\n",
    "            df['team_points'] > df[['team_attack_strength', 'team_defence_strength']].mean(axis=1) * 2\n",
    "        ).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_advanced_statistical_features(self, df):\n",
    "        \"\"\"\n",
    "        Advanced statistical features for model enhancement\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Performance consistency metrics\n",
    "        df['points_variance'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=2)\n",
    "            .var()\n",
    "            .reset_index(0, drop=True)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        \n",
    "        df['consistency_score'] = (\n",
    "            df['points_form_5'] / (df['points_variance'] + 1)  # Add 1 to avoid division by zero\n",
    "        )\n",
    "        \n",
    "        # Ceiling and floor analysis\n",
    "        df['points_ceiling'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=10, min_periods=3)\n",
    "            .max()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['points_floor'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=10, min_periods=3)\n",
    "            .min()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Risk-adjusted returns\n",
    "        df['sharpe_ratio'] = (\n",
    "            df['points_form_5'] / (df['points_variance'] + 0.1)\n",
    "        )\n",
    "        \n",
    "        # Bonus point propensity\n",
    "        df['bonus_rate'] = (\n",
    "            df.groupby('player_id')['bonus']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Minutes security\n",
    "        df['minutes_security'] = (\n",
    "            df.groupby('player_id')['minutes']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .apply(lambda x: (x >= 60).sum() / len(x))\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Yellow/red card risk\n",
    "        df['card_risk'] = (\n",
    "            df.groupby('player_id')['yellow_cards']\n",
    "            .rolling(window=10, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True)\n",
    "        ) + (\n",
    "            df.groupby('player_id')['red_cards']\n",
    "            .rolling(window=10, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True) * 3\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_seasonal_progression_features(self, df):\n",
    "        \"\"\"\n",
    "        Seasonal progression and timing features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Season phase indicators\n",
    "        df['season_phase'] = pd.cut(\n",
    "            df['gameweek'], \n",
    "            bins=[0, 10, 19, 29, 38],\n",
    "            labels=['Early', 'Autumn', 'Winter', 'Spring']\n",
    "        )\n",
    "        \n",
    "        # Gameweek timing features\n",
    "        df['gameweek_sin'] = np.sin(2 * np.pi * df['gameweek'] / 38)\n",
    "        df['gameweek_cos'] = np.cos(2 * np.pi * df['gameweek'] / 38)\n",
    "        \n",
    "        # Progressive season metrics\n",
    "        df['season_total_points'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .cumsum()\n",
    "        )\n",
    "        \n",
    "        df['season_average_points'] = (\n",
    "            df['season_total_points'] / df['gameweek']\n",
    "        )\n",
    "        \n",
    "        # Fixture congestion (Premier League context)\n",
    "        df['fixture_congestion'] = 0  # Placeholder - would calculate based on UCL/UEL participation\n",
    "        \n",
    "        # International break effect\n",
    "        df['post_international_break'] = 0  # Placeholder - would identify post-break gameweeks\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_all_features(self, players_df, teams_df, matches_df):\n",
    "        \"\"\"\n",
    "        Main method to create all features\n",
    "        \"\"\"\n",
    "        print(\"ðŸ”§ Starting comprehensive feature engineering...\")\n",
    "        \n",
    "        # Start with position-specific features\n",
    "        df = self.create_position_specific_features(players_df)\n",
    "        print(\"âœ… Position-specific features created\")\n",
    "        \n",
    "        # Add form and momentum\n",
    "        df = self.create_form_momentum_features(df)\n",
    "        print(\"âœ… Form and momentum features created\")\n",
    "        \n",
    "        # Add fixture context\n",
    "        df = self.create_fixture_context_features(df, matches_df)\n",
    "        print(\"âœ… Fixture context features created\")\n",
    "        \n",
    "        # Add value and ownership\n",
    "        df = self.create_value_ownership_features(df)\n",
    "        print(\"âœ… Value and ownership features created\")\n",
    "        \n",
    "        # Add team integration\n",
    "        df = self.create_team_integration_features(df, teams_df)\n",
    "        print(\"âœ… Team integration features created\")\n",
    "        \n",
    "        # Add advanced statistical features\n",
    "        df = self.create_advanced_statistical_features(df)\n",
    "        print(\"âœ… Advanced statistical features created\")\n",
    "        \n",
    "        # Add seasonal progression\n",
    "        df = self.create_seasonal_progression_features(df)\n",
    "        print(\"âœ… Seasonal progression features created\")\n",
    "        \n",
    "        # Add enhanced specific features (user requirements)\n",
    "        df = self.create_enhanced_form_indicators(df)\n",
    "        print(\"âœ… Enhanced 5-game form indicators created\")\n",
    "        \n",
    "        df = self.create_opposition_difficulty_ratings(df, teams_df)\n",
    "        print(\"âœ… Opposition difficulty ratings created\")\n",
    "        \n",
    "        df = self.create_fatigue_metrics(df)\n",
    "        print(\"âœ… Fatigue metrics created\")\n",
    "        \n",
    "        df = self.create_home_advantage_factors(df)\n",
    "        print(\"âœ… Home advantage factors created\")\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Feature engineering complete! Created {len(df.columns)} total features\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_feature_summary(self, df):\n",
    "        \"\"\"\n",
    "        Get summary of created features by category\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'Total Features': len(df.columns),\n",
    "            'Position-Specific': sum(len(features) for features in self.position_features.values()),\n",
    "            'Form & Momentum': len([col for col in df.columns if 'form' in col or 'trend' in col or 'momentum' in col]),\n",
    "            'Fixture Context': len([col for col in df.columns if 'difficulty' in col or 'fixture' in col]),\n",
    "            'Value & Ownership': len([col for col in df.columns if 'value' in col or 'ownership' in col or 'transfer' in col]),\n",
    "            'Team Integration': len([col for col in df.columns if 'team' in col or 'contribution' in col]),\n",
    "            'Advanced Statistical': len([col for col in df.columns if any(x in col for x in ['variance', 'consistency', 'sharpe', 'ceiling', 'floor'])]),\n",
    "            'Seasonal Progression': len([col for col in df.columns if any(x in col for x in ['season', 'gameweek_sin', 'gameweek_cos', 'phase'])]),\n",
    "            'Enhanced 5-Game Form': len([col for col in df.columns if 'form_5' in col]),\n",
    "            'Opposition Difficulty': len([col for col in df.columns if any(x in col for x in ['next_3_fixtures', 'next_5_fixtures', 'vs_difficulty'])]),\n",
    "            'Fatigue Metrics': len([col for col in df.columns if any(x in col for x in ['minutes_last', 'workload', 'rotation', 'overplay'])]),\n",
    "            'Home Advantage': len([col for col in df.columns if any(x in col for x in ['home_advantage', 'venue', 'home_form', 'away_form'])])\n",
    "        }\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_enhanced_form_indicators(self, df):\n",
    "        \"\"\"\n",
    "        Enhanced form indicators with specific focus on last 5 games performance\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(['player_id', 'gameweek'])\n",
    "        \n",
    "        # Specific 5-game form indicators (your requirement)\n",
    "        df['form_5_total_points'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['form_5_avg_points'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['form_5_goals'] = (\n",
    "            df.groupby('player_id')['goals_scored']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['form_5_assists'] = (\n",
    "            df.groupby('player_id')['assists']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['form_5_clean_sheets'] = (\n",
    "            df.groupby('player_id')['clean_sheets']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Form quality indicators\n",
    "        df['form_5_consistency'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=2)\n",
    "            .std()\n",
    "            .fillna(0)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Form trend over last 5 games\n",
    "        df['form_5_trend'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=2)\n",
    "            .apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Hot form indicator (3+ returns in last 5)\n",
    "        df['hot_form_5'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .apply(lambda x: (x > 0).sum() >= 3)\n",
    "            .astype(int)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Cold form indicator (3+ blanks in last 5)\n",
    "        df['cold_form_5'] = (\n",
    "            df.groupby('player_id')['total_points']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .apply(lambda x: (x == 0).sum() >= 3)\n",
    "            .astype(int)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_opposition_difficulty_ratings(self, df, teams_df):\n",
    "        \"\"\"\n",
    "        Enhanced opposition difficulty ratings and upcoming fixture analysis\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create team strength lookup\n",
    "        team_strength = teams_df.groupby('team_id').agg({\n",
    "            'strength_overall_home': 'mean',\n",
    "            'strength_overall_away': 'mean',\n",
    "            'strength_attack_home': 'mean',\n",
    "            'strength_attack_away': 'mean',\n",
    "            'strength_defence_home': 'mean',\n",
    "            'strength_defence_away': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Merge team strengths\n",
    "        df = df.merge(team_strength, on='team_id', how='left', suffixes=('', '_team'))\n",
    "        \n",
    "        # Enhanced difficulty metrics\n",
    "        df['opponent_defensive_strength'] = 0  # Placeholder - would get from opponent data\n",
    "        df['opponent_attacking_strength'] = 0  # Placeholder - would get from opponent data\n",
    "        \n",
    "        # Difficulty-weighted recent performance\n",
    "        df['difficulty_weighted_form_5'] = (\n",
    "            df['form_5_avg_points'] * (6 - df['difficulty']) / 5  # Boost harder fixtures\n",
    "        )\n",
    "        \n",
    "        # Next 3 fixtures difficulty (for transfer planning)\n",
    "        df['next_3_fixtures_difficulty'] = (\n",
    "            df.groupby('player_id')['difficulty']\n",
    "            .shift(-1).fillna(3) +\n",
    "            df.groupby('player_id')['difficulty']\n",
    "            .shift(-2).fillna(3) +\n",
    "            df.groupby('player_id')['difficulty']\n",
    "            .shift(-3).fillna(3)\n",
    "        ) / 3\n",
    "        \n",
    "        # Next 5 fixtures difficulty (season planning)\n",
    "        next_5_difficulties = []\n",
    "        for i in range(1, 6):\n",
    "            next_5_difficulties.append(\n",
    "                df.groupby('player_id')['difficulty']\n",
    "                .shift(-i).fillna(3)\n",
    "            )\n",
    "        df['next_5_fixtures_difficulty'] = sum(next_5_difficulties) / 5\n",
    "        \n",
    "        # Fixture difficulty swing\n",
    "        df['difficulty_improvement'] = (\n",
    "            df['difficulty'] - df['next_3_fixtures_difficulty']\n",
    "        )\n",
    "        \n",
    "        # Historical performance vs similar difficulty\n",
    "        for diff_level in [1, 2, 3, 4, 5]:\n",
    "            df[f'vs_difficulty_{diff_level}_avg'] = (\n",
    "                df[df['difficulty'] == diff_level]\n",
    "                .groupby('player_id')['total_points']\n",
    "                .transform('mean')\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_fatigue_metrics(self, df):\n",
    "        \"\"\"\n",
    "        Fatigue metrics based on minutes played recently\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(['player_id', 'gameweek'])\n",
    "        \n",
    "        # Minutes played in last 3, 5, and 10 games\n",
    "        for window in [3, 5, 10]:\n",
    "            df[f'minutes_last_{window}'] = (\n",
    "                df.groupby('player_id')['minutes']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .sum()\n",
    "                .reset_index(0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Average minutes per game\n",
    "            df[f'avg_minutes_last_{window}'] = (\n",
    "                df[f'minutes_last_{window}'] / \n",
    "                df.groupby('player_id').cumcount().clip(upper=window-1).add(1)\n",
    "            )\n",
    "        \n",
    "        # Fatigue indicators\n",
    "        df['high_minutes_workload'] = (df['minutes_last_5'] > 400).astype(int)  # 80+ min avg\n",
    "        df['rotation_risk'] = (df['minutes_last_3'] < 180).astype(int)  # Less than 60 min avg\n",
    "        df['minutes_secure'] = (df['avg_minutes_last_5'] > 75).astype(int)  # Consistent starter\n",
    "        \n",
    "        # International break recovery (placeholder)\n",
    "        df['post_international_break'] = 0  # Would be set for specific gameweeks\n",
    "        \n",
    "        # Fixture congestion (midweek games, cup competitions)\n",
    "        df['fixture_congestion_risk'] = 0  # Placeholder for European competition analysis\n",
    "        \n",
    "        # Minutes trend\n",
    "        df['minutes_trend'] = (\n",
    "            df.groupby('player_id')['minutes']\n",
    "            .rolling(window=5, min_periods=2)\n",
    "            .apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Rest advantage (games with 0 minutes indicating rest)\n",
    "        df['games_rested_last_5'] = (\n",
    "            df.groupby('player_id')['minutes']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .apply(lambda x: (x == 0).sum())\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Overplay risk (multiple 90+ minute games)\n",
    "        df['overplay_risk_last_5'] = (\n",
    "            df.groupby('player_id')['minutes']\n",
    "            .rolling(window=5, min_periods=1)\n",
    "            .apply(lambda x: (x >= 90).sum())\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_home_advantage_factors(self, df):\n",
    "        \"\"\"\n",
    "        Home advantage factors and venue-specific performance\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic home/away split (enhance existing)\n",
    "        home_mask = df['home_away'] == 'home'\n",
    "        away_mask = df['home_away'] == 'away'\n",
    "        \n",
    "        # Home vs away performance differentials\n",
    "        df['home_points_avg'] = (\n",
    "            df[home_mask].groupby('player_id')['total_points']\n",
    "            .transform('mean')\n",
    "        )\n",
    "        \n",
    "        df['away_points_avg'] = (\n",
    "            df[away_mask].groupby('player_id')['total_points']\n",
    "            .transform('mean')\n",
    "        )\n",
    "        \n",
    "        df['home_advantage'] = (\n",
    "            df['home_points_avg'] - df['away_points_avg']\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Position-specific home advantage\n",
    "        df['home_advantage_gkp'] = 0  # GKPs often better at home (familiar goal)\n",
    "        df['home_advantage_def'] = 0  # DEFs benefit from crowd/familiar backline\n",
    "        df['home_advantage_mid'] = 0  # MIDs benefit from home crowd energy\n",
    "        df['home_advantage_fwd'] = 0  # FWDs benefit from home crowd support\n",
    "        \n",
    "        # Update based on position\n",
    "        for position, advantage in [(1, 0.2), (2, 0.3), (3, 0.15), (4, 0.25)]:\n",
    "            position_mask = df['element_type'] == position\n",
    "            home_position = home_mask & position_mask\n",
    "            \n",
    "            if position == 1:\n",
    "                df.loc[home_position, 'home_advantage_gkp'] = advantage\n",
    "            elif position == 2:\n",
    "                df.loc[home_position, 'home_advantage_def'] = advantage\n",
    "            elif position == 3:\n",
    "                df.loc[home_position, 'home_advantage_mid'] = advantage\n",
    "            elif position == 4:\n",
    "                df.loc[home_position, 'home_advantage_fwd'] = advantage\n",
    "        \n",
    "        # Home fixture run (next N home games)\n",
    "        df['next_3_home_games'] = 0  # Count of home games in next 3\n",
    "        df['next_5_home_games'] = 0  # Count of home games in next 5\n",
    "        \n",
    "        # Away fixture run\n",
    "        df['next_3_away_games'] = 0  # Count of away games in next 3\n",
    "        df['next_5_away_games'] = 0  # Count of away games in next 5\n",
    "        \n",
    "        # Home/away streaks\n",
    "        df['current_home_streak'] = 0  # Consecutive home games\n",
    "        df['current_away_streak'] = 0  # Consecutive away games\n",
    "        \n",
    "        # Venue-specific form\n",
    "        for venue in ['home', 'away']:\n",
    "            venue_mask = df['home_away'] == venue\n",
    "            df[f'{venue}_form_last_3'] = (\n",
    "                df[venue_mask].groupby('player_id')['total_points']\n",
    "                .rolling(window=3, min_periods=1)\n",
    "                .mean()\n",
    "                .reindex(df.index)\n",
    "                .fillna(df['total_points'])  # Fill with current game if no history\n",
    "            )\n",
    "        \n",
    "        # Venue comfort rating\n",
    "        df['venue_comfort'] = np.where(\n",
    "            home_mask, \n",
    "            df['home_form_last_3'],\n",
    "            df['away_form_last_3']\n",
    "        )\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7367bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Implementation Example - Load Real 2025/26 Data\n",
    "def load_and_process_current_season():\n",
    "    \"\"\"\n",
    "    Load and process the current 2025/26 season data with 4 rounds completed\n",
    "    \"\"\"\n",
    "    # Initialize feature engineer\n",
    "    feature_engineer = FootballFeatureEngineer()\n",
    "    \n",
    "    # Load data from our database\n",
    "    players_df, teams_df, matches_df = feature_engineer.load_data_from_db()\n",
    "    \n",
    "    print(f\"ðŸ“Š Loaded data:\")\n",
    "    print(f\"   â€¢ Players records: {len(players_df)}\")\n",
    "    print(f\"   â€¢ Teams records: {len(teams_df)}\")\n",
    "    print(f\"   â€¢ Matches records: {len(matches_df)}\")\n",
    "    print(f\"   â€¢ Gameweeks available: {players_df['gameweek'].nunique()}\")\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    engineered_df = feature_engineer.create_all_features(players_df, teams_df, matches_df)\n",
    "    \n",
    "    # Feature summary\n",
    "    summary = feature_engineer.get_feature_summary(engineered_df)\n",
    "    print(\"\\nðŸ“ˆ Feature Engineering Summary:\")\n",
    "    for category, count in summary.items():\n",
    "        print(f\"   â€¢ {category}: {count}\")\n",
    "    \n",
    "    return engineered_df, feature_engineer\n",
    "\n",
    "# Execute the feature engineering\n",
    "# engineered_data, feature_eng = load_and_process_current_season()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbab20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_position_specific_performance(df, position='FWD'):\n",
    "    \"\"\"\n",
    "    Detailed analysis of position-specific features\n",
    "    \"\"\"\n",
    "    position_data = df[df['position_name'] == position].copy()\n",
    "    \n",
    "    print(f\"ðŸŽ¯ {position} Performance Analysis:\")\n",
    "    print(f\"   â€¢ Total players: {position_data['player_id'].nunique()}\")\n",
    "    print(f\"   â€¢ Data points: {len(position_data)}\")\n",
    "    \n",
    "    if position == 'FWD':\n",
    "        key_metrics = ['goals_per_90', 'shot_conversion_rate', 'attacking_threat', \n",
    "                      'clinical_finishing', 'fwd_differential_score']\n",
    "    elif position == 'MID':\n",
    "        key_metrics = ['creativity_index', 'box_to_box_rating', 'attacking_mid_potential',\n",
    "                      'key_passes_per_90', 'work_rate_index']\n",
    "    elif position == 'DEF':\n",
    "        key_metrics = ['defensive_reliability', 'attacking_threat_def', 'def_bonus_potential',\n",
    "                      'defensive_actions_per_90', 'pass_accuracy']\n",
    "    elif position == 'GKP':\n",
    "        key_metrics = ['save_percentage', 'gkp_value_rating', 'clean_sheet_rate',\n",
    "                      'saves_per_90', 'distribution_accuracy']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Key {position} Metrics (Top 5 players):\")\n",
    "    for metric in key_metrics:\n",
    "        if metric in position_data.columns:\n",
    "            top_performers = (position_data.groupby('web_name')[metric]\n",
    "                            .mean()\n",
    "                            .sort_values(ascending=False)\n",
    "                            .head(5))\n",
    "            print(f\"\\n{metric}:\")\n",
    "            for player, value in top_performers.items():\n",
    "                print(f\"   â€¢ {player}: {value:.2f}\")\n",
    "    \n",
    "    return position_data\n",
    "\n",
    "def create_feature_correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze feature correlations with target variable (total_points)\n",
    "    \"\"\"\n",
    "    # Select numeric features only\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove id columns and target\n",
    "    feature_cols = [col for col in numeric_features \n",
    "                   if not any(x in col.lower() for x in ['id', 'index'])]\n",
    "    \n",
    "    if 'total_points' in feature_cols:\n",
    "        # Calculate correlations with total_points\n",
    "        correlations = df[feature_cols].corr()['total_points'].abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"ðŸ”— Top 20 Features Correlated with Total Points:\")\n",
    "        for i, (feature, corr) in enumerate(correlations.head(20).items()):\n",
    "            if feature != 'total_points':\n",
    "                print(f\"   {i+1:2d}. {feature}: {corr:.3f}\")\n",
    "        \n",
    "        return correlations\n",
    "    \n",
    "    return None\n",
    "\n",
    "def prepare_features_for_modeling(df, target_col='total_points'):\n",
    "    \"\"\"\n",
    "    Prepare final feature set for machine learning models\n",
    "    \"\"\"\n",
    "    # Remove non-predictive columns\n",
    "    exclude_cols = [\n",
    "        'player_id', 'web_name', 'team_id', 'gameweek', 'match_id',\n",
    "        'first_name', 'second_name', 'position_name', 'season_phase',\n",
    "        target_col  # Remove target from features\n",
    "    ]\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = df[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Prepare final dataset\n",
    "    features_df = df[feature_cols].copy()\n",
    "    \n",
    "    # Fill missing values\n",
    "    features_df = features_df.fillna(features_df.median())\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Final Feature Set Prepared:\")\n",
    "    print(f\"   â€¢ Total features: {len(feature_cols)}\")\n",
    "    print(f\"   â€¢ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"   â€¢ Numeric features: {len(feature_cols) - len(categorical_cols)}\")\n",
    "    print(f\"   â€¢ Target variable: {target_col}\")\n",
    "    print(f\"   â€¢ Data shape: {features_df.shape}\")\n",
    "    \n",
    "    return features_df, df[target_col], feature_cols, categorical_cols\n",
    "\n",
    "# Example usage for the current 2025/26 season analysis\n",
    "def run_feature_engineering_demo():\n",
    "    \"\"\"\n",
    "    Demonstration of the complete feature engineering pipeline\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Running Feature Engineering Demo for 2025/26 Season\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load and process data\n",
    "    engineered_data, feature_eng = load_and_process_current_season()\n",
    "    \n",
    "    # Analyze each position\n",
    "    for position in ['FWD', 'MID', 'DEF', 'GKP']:\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        analyze_position_specific_performance(engineered_data, position)\n",
    "    \n",
    "    # Feature correlation analysis\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    correlations = create_feature_correlation_analysis(engineered_data)\n",
    "    \n",
    "    # Prepare for modeling\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    X, y, feature_names, categorical_features = prepare_features_for_modeling(engineered_data)\n",
    "    \n",
    "    print(f\"\\nâœ… Feature engineering complete and ready for modeling!\")\n",
    "    \n",
    "    return X, y, feature_names, categorical_features, engineered_data\n",
    "\n",
    "# Uncomment to run the demo\n",
    "# X, y, feature_names, categorical_features, full_data = run_feature_engineering_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f20cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Feature Demonstration for Your Specific Requirements\n",
    "def demonstrate_enhanced_features():\n",
    "    \"\"\"\n",
    "    Demonstrate the specific enhanced features requested:\n",
    "    1. Form indicators (last 5 games performance)\n",
    "    2. Opposition difficulty ratings  \n",
    "    3. Fatigue metrics (minutes played recently)\n",
    "    4. Home advantage factors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ Demonstrating Enhanced Features for 2025/26 Season\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load sample data\n",
    "    players_df = pd.read_csv('sample_players_data.csv')\n",
    "    \n",
    "    # Create mock teams and matches data\n",
    "    teams_data = []\n",
    "    matches_data = []\n",
    "    \n",
    "    for gw in range(1, 5):  # 4 gameweeks\n",
    "        for team_id in range(1, 21):  # 20 teams\n",
    "            teams_data.append({\n",
    "                'gameweek': gw,\n",
    "                'team_id': team_id,\n",
    "                'strength_overall_home': np.random.uniform(1000, 1400),\n",
    "                'strength_overall_away': np.random.uniform(900, 1300),\n",
    "                'strength_attack_home': np.random.uniform(1000, 1400),\n",
    "                'strength_attack_away': np.random.uniform(900, 1300),\n",
    "                'strength_defence_home': np.random.uniform(1000, 1400),\n",
    "                'strength_defence_away': np.random.uniform(900, 1300)\n",
    "            })\n",
    "            \n",
    "            matches_data.append({\n",
    "                'gameweek': gw,\n",
    "                'team_id': team_id,\n",
    "                'difficulty': np.random.randint(1, 6),\n",
    "                'home_away': np.random.choice(['home', 'away']),\n",
    "                'opponent': f'Team_{np.random.randint(1, 21)}'\n",
    "            })\n",
    "    \n",
    "    teams_df = pd.DataFrame(teams_data)\n",
    "    matches_df = pd.DataFrame(matches_data)\n",
    "    \n",
    "    # Apply enhanced feature engineering\n",
    "    engineer = FootballFeatureEngineer()\n",
    "    \n",
    "    print(\"\\\\nðŸ”§ Applying Enhanced Feature Engineering...\")\n",
    "    \n",
    "    # Test each new feature set\n",
    "    df_enhanced = engineer.create_position_specific_features(players_df)\n",
    "    print(\"âœ… Base position features applied\")\n",
    "    \n",
    "    df_enhanced = engineer.create_enhanced_form_indicators(df_enhanced)\n",
    "    print(\"âœ… Enhanced 5-game form indicators created\")\n",
    "    \n",
    "    df_enhanced = engineer.create_opposition_difficulty_ratings(df_enhanced, teams_df)\n",
    "    print(\"âœ… Opposition difficulty ratings created\")\n",
    "    \n",
    "    df_enhanced = engineer.create_fatigue_metrics(df_enhanced)\n",
    "    print(\"âœ… Fatigue metrics created\")\n",
    "    \n",
    "    df_enhanced = engineer.create_home_advantage_factors(df_enhanced)\n",
    "    print(\"âœ… Home advantage factors created\")\n",
    "    \n",
    "    print(f\"\\\\nðŸ“Š Enhanced Features Summary:\")\n",
    "    print(f\"   â€¢ Original columns: {len(players_df.columns)}\")\n",
    "    print(f\"   â€¢ Enhanced columns: {len(df_enhanced.columns)}\")\n",
    "    print(f\"   â€¢ New features added: {len(df_enhanced.columns) - len(players_df.columns)}\")\n",
    "    \n",
    "    # Demonstrate specific features\n",
    "    print(\"\\\\nðŸŽ¯ 1. Form Indicators (Last 5 Games):\")\n",
    "    form_features = [col for col in df_enhanced.columns if 'form_5' in col]\n",
    "    print(f\"   â€¢ Created {len(form_features)} form indicators:\")\n",
    "    for feature in form_features[:5]:  # Show first 5\n",
    "        avg_val = df_enhanced[feature].mean()\n",
    "        print(f\"     - {feature}: avg {avg_val:.2f}\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ 2. Opposition Difficulty Ratings:\")\n",
    "    difficulty_features = [col for col in df_enhanced.columns if any(x in col for x in ['next_3_fixtures', 'next_5_fixtures', 'difficulty_weighted'])]\n",
    "    print(f\"   â€¢ Created {len(difficulty_features)} difficulty features:\")\n",
    "    for feature in difficulty_features:\n",
    "        avg_val = df_enhanced[feature].mean()\n",
    "        print(f\"     - {feature}: avg {avg_val:.2f}\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ 3. Fatigue Metrics:\")\n",
    "    fatigue_features = [col for col in df_enhanced.columns if any(x in col for x in ['minutes_last', 'workload', 'rotation', 'overplay'])]\n",
    "    print(f\"   â€¢ Created {len(fatigue_features)} fatigue features:\")\n",
    "    for feature in fatigue_features:\n",
    "        avg_val = df_enhanced[feature].mean()\n",
    "        print(f\"     - {feature}: avg {avg_val:.2f}\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ 4. Home Advantage Factors:\")\n",
    "    home_features = [col for col in df_enhanced.columns if any(x in col for x in ['home_advantage', 'venue', 'home_form', 'away_form'])]\n",
    "    print(f\"   â€¢ Created {len(home_features)} home advantage features:\")\n",
    "    for feature in home_features:\n",
    "        avg_val = df_enhanced[feature].mean()\n",
    "        print(f\"     - {feature}: avg {avg_val:.2f}\")\n",
    "    \n",
    "    # Show some practical insights\n",
    "    print(\"\\\\nðŸ’¡ Practical Insights from Enhanced Features:\")\n",
    "    \n",
    "    # Players with best 5-game form\n",
    "    latest_gw = df_enhanced['gameweek'].max()\n",
    "    latest_data = df_enhanced[df_enhanced['gameweek'] == latest_gw]\n",
    "    \n",
    "    if 'form_5_avg_points' in latest_data.columns:\n",
    "        top_form = latest_data.nlargest(5, 'form_5_avg_points')[['web_name', 'position_name', 'form_5_avg_points']]\n",
    "        print(\"\\\\nðŸ”¥ Top 5-Game Form Players:\")\n",
    "        for _, player in top_form.iterrows():\n",
    "            print(f\"   â€¢ {player['web_name']} ({player['position_name']}): {player['form_5_avg_points']:.2f} pts/game\")\n",
    "    \n",
    "    # Players with easiest upcoming fixtures\n",
    "    if 'next_3_fixtures_difficulty' in latest_data.columns:\n",
    "        easy_fixtures = latest_data.nsmallest(5, 'next_3_fixtures_difficulty')[['web_name', 'position_name', 'next_3_fixtures_difficulty']]\n",
    "        print(\"\\\\nðŸŽ¯ Easiest Next 3 Fixtures:\")\n",
    "        for _, player in easy_fixtures.iterrows():\n",
    "            print(f\"   â€¢ {player['web_name']} ({player['position_name']}): {player['next_3_fixtures_difficulty']:.2f} difficulty\")\n",
    "    \n",
    "    # Players with rotation risk\n",
    "    if 'rotation_risk' in latest_data.columns:\n",
    "        rotation_risk = latest_data[latest_data['rotation_risk'] == 1][['web_name', 'position_name', 'avg_minutes_last_3']]\n",
    "        print(f\"\\\\nâš ï¸  Rotation Risk Players: {len(rotation_risk)}\")\n",
    "        if len(rotation_risk) > 0:\n",
    "            for _, player in rotation_risk.head(3).iterrows():\n",
    "                print(f\"   â€¢ {player['web_name']} ({player['position_name']}): {player['avg_minutes_last_3']:.1f} min/game\")\n",
    "    \n",
    "    print(\"\\\\nâœ… Enhanced Feature Demonstration Complete!\")\n",
    "    return df_enhanced\n",
    "\n",
    "# Uncomment to run the demonstration\n",
    "# enhanced_data = demonstrate_enhanced_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86990192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocess football data for machine learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def prepare_data(self, df, target_col, categorical_cols=None, date_col='date'):\n",
    "        \"\"\"\n",
    "        Prepare data for machine learning\n",
    "        \"\"\"\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                if col in processed_df.columns:\n",
    "                    le = LabelEncoder()\n",
    "                    processed_df[col] = le.fit_transform(processed_df[col].astype(str))\n",
    "                    self.label_encoders[col] = le\n",
    "        \n",
    "        # Separate features and target\n",
    "        feature_cols = [col for col in processed_df.columns \n",
    "                       if col not in [target_col, date_col, 'player_id', 'match_id']]\n",
    "        \n",
    "        X = processed_df[feature_cols]\n",
    "        y = processed_df[target_col]\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "    def split_data_temporal(self, X, y, df, date_col='date', test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split data temporally (important for time series)\n",
    "        \"\"\"\n",
    "        # Sort by date\n",
    "        dates = pd.to_datetime(df[date_col])\n",
    "        split_date = dates.quantile(1 - test_size)\n",
    "        \n",
    "        train_mask = dates < split_date\n",
    "        test_mask = dates >= split_date\n",
    "        \n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        \n",
    "        # Further split training into train/validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def scale_features(self, X_train, X_val, X_test):\n",
    "        \"\"\"\n",
    "        Scale features using training data statistics\n",
    "        \"\"\"\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "# TODO: Load and prepare your data\n",
    "# preprocessor = FootballDataPreprocessor()\n",
    "# X, y, feature_cols = preprocessor.prepare_data(data, 'next_match_rating')\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data_temporal(X, y, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789a24b",
   "metadata": {},
   "source": [
    "## 2. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae52d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModels:\n",
    "    \"\"\"\n",
    "    Implement baseline models for comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scores = {}\n",
    "    \n",
    "    def train_linear_models(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train linear regression models\n",
    "        \"\"\"\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0),\n",
    "            'Lasso Regression': Lasso(alpha=0.1)\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            self.models[name] = model\n",
    "            self.scores[name] = {'MSE': mse, 'MAE': mae, 'R2': r2}\n",
    "            \n",
    "            print(f\"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    def train_tree_models(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train tree-based models\n",
    "        \"\"\"\n",
    "        models = {\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            self.models[name] = model\n",
    "            self.scores[name] = {'MSE': mse, 'MAE': mae, 'R2': r2}\n",
    "            \n",
    "            print(f\"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# TODO: Train baseline models\n",
    "# baseline = BaselineModels()\n",
    "# baseline.train_linear_models(X_train, y_train, X_val, y_val)\n",
    "# baseline.train_tree_models(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52976bd4",
   "metadata": {},
   "source": [
    "## 3. Advanced Models - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e73346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel:\n",
    "    \"\"\"\n",
    "    XGBoost model with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, params=None):\n",
    "        \"\"\"\n",
    "        Train XGBoost model\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            params = {\n",
    "                'n_estimators': 1000,\n",
    "                'max_depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        \n",
    "        self.model = xgb.XGBRegressor(**params)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_feature_importance(self, feature_names):\n",
    "        \"\"\"\n",
    "        Get feature importance\n",
    "        \"\"\"\n",
    "        importance = self.model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return feature_importance\n",
    "    \n",
    "    def plot_feature_importance(self, feature_names, top_n=20):\n",
    "        \"\"\"\n",
    "        Plot feature importance\n",
    "        \"\"\"\n",
    "        feature_importance = self.get_feature_importance(feature_names)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=feature_importance.head(top_n), \n",
    "                   x='importance', y='feature')\n",
    "        plt.title('XGBoost Feature Importance')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance\n",
    "\n",
    "# TODO: Train XGBoost model\n",
    "# xgb_model = XGBoostModel()\n",
    "# xgb_model.train(X_train, y_train, X_val, y_val)\n",
    "# xgb_predictions = xgb_model.predict(X_test)\n",
    "# xgb_importance = xgb_model.plot_feature_importance(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23a6a5",
   "metadata": {},
   "source": [
    "## 4. Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ff2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel:\n",
    "    \"\"\"\n",
    "    Deep learning model for player performance prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def build_model(self, hidden_layers=[128, 64, 32], dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Build neural network architecture\n",
    "        \"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(keras.layers.Dense(hidden_layers[0], \n",
    "                                   input_dim=self.input_dim, \n",
    "                                   activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for units in hidden_layers[1:]:\n",
    "            model.add(keras.layers.Dense(units, activation='relu'))\n",
    "            model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(keras.layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', \n",
    "                     loss='mse', \n",
    "                     metrics=['mae'])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \"\"\"\n",
    "        # Callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return self.model.predict(X).flatten()\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(self.history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # MAE\n",
    "        ax2.plot(self.history.history['mae'], label='Training MAE')\n",
    "        ax2.plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "        ax2.set_title('Model MAE')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('MAE')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# TODO: Train neural network\n",
    "# nn_model = NeuralNetworkModel(input_dim=X_train.shape[1])\n",
    "# nn_model.build_model()\n",
    "# nn_history = nn_model.train(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "# nn_predictions = nn_model.predict(X_test_scaled)\n",
    "# nn_model.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cee81",
   "metadata": {},
   "source": [
    "## 5. Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModels:\n",
    "    \"\"\"\n",
    "    Time series models for player performance forecasting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prophet_models = {}\n",
    "        self.lstm_model = None\n",
    "    \n",
    "    def train_prophet_per_player(self, df, player_col='player_id', \n",
    "                                date_col='date', target_col='rating'):\n",
    "        \"\"\"\n",
    "        Train Prophet model for each player\n",
    "        \"\"\"\n",
    "        players = df[player_col].unique()\n",
    "        \n",
    "        for player in players[:10]:  # Limit to first 10 players for demo\n",
    "            player_data = df[df[player_col] == player].copy()\n",
    "            \n",
    "            if len(player_data) < 10:  # Need minimum data points\n",
    "                continue\n",
    "            \n",
    "            # Prepare data for Prophet\n",
    "            prophet_data = player_data[[date_col, target_col]].copy()\n",
    "            prophet_data.columns = ['ds', 'y']\n",
    "            prophet_data['ds'] = pd.to_datetime(prophet_data['ds'])\n",
    "            \n",
    "            # Train Prophet model\n",
    "            model = Prophet(\n",
    "                daily_seasonality=False,\n",
    "                weekly_seasonality=True,\n",
    "                yearly_seasonality=False,\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            model.fit(prophet_data)\n",
    "            self.prophet_models[player] = model\n",
    "            \n",
    "            print(f\"Trained Prophet model for player {player}\")\n",
    "        \n",
    "        return self.prophet_models\n",
    "    \n",
    "    def predict_prophet(self, player_id, future_dates):\n",
    "        \"\"\"\n",
    "        Make predictions using Prophet model\n",
    "        \"\"\"\n",
    "        if player_id not in self.prophet_models:\n",
    "            return None\n",
    "        \n",
    "        model = self.prophet_models[player_id]\n",
    "        \n",
    "        # Create future dataframe\n",
    "        future_df = pd.DataFrame({'ds': pd.to_datetime(future_dates)})\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast = model.predict(future_df)\n",
    "        \n",
    "        return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "    \n",
    "    def create_lstm_sequences(self, data, sequence_length=10):\n",
    "        \"\"\"\n",
    "        Create sequences for LSTM training\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:(i + sequence_length)])\n",
    "            y.append(data[i + sequence_length])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_lstm_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model for time series prediction\n",
    "        \"\"\"\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.LSTM(50, return_sequences=False),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(25),\n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        \n",
    "        self.lstm_model = model\n",
    "        return model\n",
    "\n",
    "# TODO: Train time series models\n",
    "# ts_models = TimeSeriesModels()\n",
    "# prophet_models = ts_models.train_prophet_per_player(player_time_series_data)\n",
    "# \n",
    "# # For LSTM\n",
    "# sequence_length = 10\n",
    "# X_lstm, y_lstm = ts_models.create_lstm_sequences(player_ratings, sequence_length)\n",
    "# lstm_model = ts_models.build_lstm_model((sequence_length, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfec2a",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test, model_names):\n",
    "    \"\"\"\n",
    "    Evaluate and compare multiple models\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, (model, name) in enumerate(zip(models, model_names)):\n",
    "        if hasattr(model, 'predict'):\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            y_pred = model(X_test)  # For custom prediction functions\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,\n",
    "            'RMSE': np.sqrt(mse)\n",
    "        })\n",
    "        \n",
    "        print(f\"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"\n",
    "    Plot model comparison\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['MSE', 'MAE', 'R2', 'RMSE']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        sns.barplot(data=results_df, x='Model', y=metric, ax=ax)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_vs_actual(y_true, y_pred, title=\"Predictions vs Actual\"):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Evaluate all models\n",
    "# all_models = [baseline.models['Random Forest'], xgb_model.model, nn_model.model]\n",
    "# model_names = ['Random Forest', 'XGBoost', 'Neural Network']\n",
    "# results = evaluate_models(all_models, X_test, y_test, model_names)\n",
    "# plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e2ff3",
   "metadata": {},
   "source": [
    "## 7. Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    \"\"\"\n",
    "    Ensemble multiple models for better predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None):\n",
    "        self.models = models\n",
    "        self.weights = weights if weights else [1/len(models)] * len(models)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make ensemble predictions\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            if hasattr(model, 'predict'):\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=self.weights)\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def optimize_weights(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Optimize ensemble weights using validation data\n",
    "        \"\"\"\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        def objective(weights):\n",
    "            # Normalize weights\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = []\n",
    "            for model in self.models:\n",
    "                if hasattr(model, 'predict'):\n",
    "                    pred = model.predict(X_val)\n",
    "                else:\n",
    "                    pred = model(X_val)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
    "            mse = mean_squared_error(y_val, ensemble_pred)\n",
    "            \n",
    "            return mse\n",
    "        \n",
    "        # Initial weights\n",
    "        initial_weights = np.array([1/len(self.models)] * len(self.models))\n",
    "        \n",
    "        # Constraints (weights sum to 1)\n",
    "        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "        bounds = [(0, 1) for _ in range(len(self.models))]\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', \n",
    "                         bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        self.weights = result.x\n",
    "        print(f\"Optimized weights: {self.weights}\")\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "# TODO: Create ensemble model\n",
    "# ensemble = EnsembleModel([baseline.models['Random Forest'], xgb_model.model, nn_model.model])\n",
    "# optimized_weights = ensemble.optimize_weights(X_val, y_val)\n",
    "# ensemble_predictions = ensemble.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
